{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4f214bcc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pandas\n",
      "  Using cached pandas-2.3.3-cp311-cp311-macosx_11_0_arm64.whl.metadata (91 kB)\n",
      "Requirement already satisfied: numpy>=1.23.2 in /Users/anubhavtomar/Library/Caches/pypoetry/virtualenvs/ai-agent-eval-E67KMgol-py3.11/lib/python3.11/site-packages (from pandas) (2.4.0)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /Users/anubhavtomar/Library/Caches/pypoetry/virtualenvs/ai-agent-eval-E67KMgol-py3.11/lib/python3.11/site-packages (from pandas) (2.9.0.post0)\n",
      "Collecting pytz>=2020.1 (from pandas)\n",
      "  Using cached pytz-2025.2-py2.py3-none-any.whl.metadata (22 kB)\n",
      "Collecting tzdata>=2022.7 (from pandas)\n",
      "  Downloading tzdata-2025.3-py2.py3-none-any.whl.metadata (1.4 kB)\n",
      "Requirement already satisfied: six>=1.5 in /Users/anubhavtomar/Library/Caches/pypoetry/virtualenvs/ai-agent-eval-E67KMgol-py3.11/lib/python3.11/site-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
      "Using cached pandas-2.3.3-cp311-cp311-macosx_11_0_arm64.whl (10.8 MB)\n",
      "Using cached pytz-2025.2-py2.py3-none-any.whl (509 kB)\n",
      "Downloading tzdata-2025.3-py2.py3-none-any.whl (348 kB)\n",
      "Installing collected packages: pytz, tzdata, pandas\n",
      "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m3/3\u001b[0m [pandas]2m2/3\u001b[0m [pandas]\n",
      "\u001b[1A\u001b[2KSuccessfully installed pandas-2.3.3 pytz-2025.2 tzdata-2025.3\n"
     ]
    }
   ],
   "source": [
    "!pip install pandas\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f15bb5b",
   "metadata": {},
   "source": [
    "## Testing Methodology & Scenario Design\n",
    "\n",
    "### Real-World Agent Simulation\n",
    "**Goal**: Demonstrate evaluation on realistic agent usage patterns\n",
    "\n",
    "**Agent Profile**: Travel Assistant v1\n",
    "- Fixed prompt: `artifacts/prompts/prompt_v1.txt`\n",
    "- Consistent persona: Helpful travel concierge\n",
    "\n",
    "### Realistic Conversation Scenarios\n",
    "**Based on actual travel assistant usage**:\n",
    "- Flight bookings with passenger preferences\n",
    "- Hotel reservations with amenity requirements\n",
    "- Multi-step conversations with context\n",
    "- Error handling and clarification requests\n",
    "- Price comparisons and recommendations\n",
    "\n",
    "**User Types**: Business travelers, vacation planners, budget-conscious tourists\n",
    "\n",
    "### Pattern Embedding Strategy\n",
    "**Challenge**: Create detectable patterns without obvious artificiality\n",
    "\n",
    "**Approach**:\n",
    "1. **Natural Failure Points**: Use realistic scenarios where agents commonly fail\n",
    "2. **Subtle Issues**: Problems that seem minor but indicate systematic issues\n",
    "3. **Pattern Density**: 4-5 conversations per pattern for reliable clustering\n",
    "4. **Mixed Severity**: Range from obvious errors to nuanced preferences\n",
    "\n",
    "**Embedding Techniques**:\n",
    "- Date format variations that seem like user input errors\n",
    "- Partial preference mentions that agents might overlook\n",
    "- Contextual requirements hidden in conversation flow\n",
    "- Tool parameter combinations that test validation logic\n",
    "\n",
    "### Fixed Prompt Design\n",
    "**Why Fixed**: Control variable to isolate evaluation system performance\n",
    "- Same prompt across all conversations\n",
    "- No prompt variations or A/B testing\n",
    "- Focus on evaluation pipeline, not prompt optimization\n",
    "- Enables comparison of evaluation results across identical agent behavior\n",
    "\n",
    "### Evaluation Objectives\n",
    "- **Pattern Recognition**: Can the system detect embedded failure patterns?\n",
    "- **Semantic Depth**: Does LLM judge catch issues heuristics miss?\n",
    "- **Proposal Quality**: Are generated suggestions actionable and relevant?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d338d7fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.pipeline.batch_processor import BatchPipelineProcessor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6bd22675",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "‚öôÔ∏è Configuration:\n",
      "   Pattern: *.json (single agent version)\n",
      "   Stages: Intelligent defaults (Heuristic ‚Üí LLM ‚Üí Analysis)\n",
      "   Features: One-by-one progress, staged evaluation, pattern analysis\n",
      "üöÄ Starting Batch Analysis Pipeline\n",
      "==================================================\n",
      "\n",
      "üìÅ Stage 1: Loading Conversations\n",
      "   Looking in directory: /Users/anubhavtomar/Anubhav/repos/agent-eval/data/travel_agent\n",
      "   Using pattern: travel_v1_*.json\n",
      "üìñ Found 15 files matching 'travel_v1_*.json'\n",
      "    1/15 ‚úÖ travel_v1_good_001.json   ‚Üí 1 conversation\n",
      "    2/15 ‚úÖ travel_v1_issue_007.json  ‚Üí 1 conversation\n",
      "    3/15 ‚úÖ travel_v1_issue_011.json  ‚Üí 1 conversation\n",
      "    4/15 ‚úÖ travel_v1_issue_010.json  ‚Üí 1 conversation\n",
      "    5/15 ‚úÖ travel_v1_issue_006.json  ‚Üí 1 conversation\n",
      "    6/15 ‚úÖ travel_v1_issue_001.json  ‚Üí 1 conversation\n",
      "    7/15 ‚úÖ travel_v1_issue_003.json  ‚Üí 1 conversation\n",
      "    8/15 ‚úÖ travel_v1_issue_002.json  ‚Üí 1 conversation\n",
      "    9/15 ‚úÖ travel_v1_issue_009.json  ‚Üí 1 conversation\n",
      "   10/15 ‚úÖ travel_v1_good_003.json   ‚Üí 1 conversation\n",
      "   11/15 ‚úÖ travel_v1_issue_005.json  ‚Üí 1 conversation\n",
      "   12/15 ‚úÖ travel_v1_issue_004.json  ‚Üí 1 conversation\n",
      "   13/15 ‚úÖ travel_v1_issue_012.json  ‚Üí 1 conversation\n",
      "   14/15 ‚úÖ travel_v1_good_002.json   ‚Üí 1 conversation\n",
      "   15/15 ‚úÖ travel_v1_issue_008.json  ‚Üí 1 conversation\n",
      "üì• Loaded 15 conversations total\n",
      "\n",
      "üîÑ Stage 2: Ingesting Conversations\n",
      "‚ùå Ingestion failed: 'IngestionResult' object has no attribute 'get'\n",
      "\n",
      "üîÑ Stage 3: Staged Evaluation\n",
      "\n",
      "üîÑ Stage 3: heuristic_stage\n",
      "   Fast rule-based evaluators on all conversations\n",
      "   Evaluators: heuristic, tool_call, tool_causality\n",
      "   üìä Processing 15 conversations...\n",
      "       1/15 ‚úÖ travel_v1_good_001           Score: 1.00 Issues: 0\n",
      "       2/15 ‚ùå travel_v1_issue_007          Score: 0.94 Issues: 2\n",
      "       3/15 ‚ùå travel_v1_issue_011          Score: 0.94 Issues: 1\n",
      "       4/15 ‚úÖ travel_v1_issue_010          Score: 1.00 Issues: 0\n",
      "       5/15 ‚ùå travel_v1_issue_006          Score: 0.81 Issues: 2\n",
      "       6/15 ‚ùå travel_v1_issue_001          Score: 0.71 Issues: 3\n",
      "       7/15 ‚ùå travel_v1_issue_003          Score: 0.88 Issues: 1\n",
      "       8/15 ‚ùå travel_v1_issue_002          Score: 0.94 Issues: 1\n",
      "       9/15 ‚ùå travel_v1_issue_009          Score: 0.94 Issues: 1\n",
      "      10/15 ‚ùå travel_v1_good_003           Score: 0.97 Issues: 1\n",
      "      11/15 ‚ùå travel_v1_issue_005          Score: 0.97 Issues: 1\n",
      "      12/15 ‚ùå travel_v1_issue_004          Score: 0.69 Issues: 3\n",
      "      13/15 ‚úÖ travel_v1_issue_012          Score: 1.00 Issues: 0\n",
      "      14/15 ‚úÖ travel_v1_good_002           Score: 1.00 Issues: 0\n",
      "      15/15 ‚ùå travel_v1_issue_008          Score: 0.71 Issues: 3\n",
      "   ‚úÖ Stage 3 complete: 15 evaluated, 11 with issues\n",
      "\n",
      "üîÑ Stage 4: llm_stage\n",
      "   LLM evaluation on conversations with issues\n",
      "   Evaluators: llm_judge\n",
      "   üîç Filtered to 11 conversations with issues\n",
      "   üìä Processing 11 conversations...\n",
      "       1/11 ‚ùå travel_v1_issue_007          Score: 0.67 Issues: 2\n",
      "       2/11 ‚ùå travel_v1_issue_011          Score: 0.57 Issues: 2\n",
      "       3/11 ‚ùå travel_v1_issue_006          Score: 0.73 Issues: 1\n",
      "       4/11 ‚ùå travel_v1_issue_001          Score: 0.57 Issues: 2\n",
      "       5/11 ‚ùå travel_v1_issue_003          Score: 0.50 Issues: 2\n",
      "       6/11 ‚ùå travel_v1_issue_002          Score: 0.73 Issues: 2\n",
      "       7/11 ‚ùå travel_v1_issue_009          Score: 0.50 Issues: 2\n",
      "       8/11 ‚ùå travel_v1_good_003           Score: 0.92 Issues: 1\n",
      "       9/11 ‚ùå travel_v1_issue_005          Score: 0.67 Issues: 2\n",
      "      10/11 ‚ùå travel_v1_issue_004          Score: 0.50 Issues: 2\n",
      "      11/11 ‚ùå travel_v1_issue_008          Score: 0.50 Issues: 2\n",
      "   ‚úÖ Stage 4 complete: 11 evaluated, 11 with issues\n",
      "\n",
      "üîÑ Final Stage: Analysis\n",
      "   üìà Analyzing evaluations for patterns...\n",
      "DEBUG: Analysis discovery found 11 evaluations with issues out of 15 reviewed.\n",
      "DEBUG: Clustering 20 granular issues...\n",
      "DEBUG: Enriching cluster 1/5 with LLM reasoning...\n",
      "DEBUG: Enriching cluster 2/5 with LLM reasoning...\n",
      "DEBUG: Enriching cluster 3/5 with LLM reasoning...\n",
      "DEBUG: Enriching cluster 4/5 with LLM reasoning...\n",
      "DEBUG: Enriching cluster 5/5 with LLM reasoning...\n",
      "DEBUG: Discovery phase identified 5 distinct failure patterns.\n",
      "DEBUG: Generating improvement proposal for pattern: 'Incomplete Information Handling' (Significance: 120.0)\n",
      "DEBUG: Generating improvement proposal for pattern: 'Incomplete Flight Guidance' (Significance: 7.5)\n",
      "DEBUG: Generating improvement proposal for pattern: 'Budget and Preference Ignorance' (Significance: 17.0)\n",
      "DEBUG: Generating improvement proposal for pattern: 'Budget and Preferences Ignored' (Significance: 6.5)\n",
      "DEBUG: Generating improvement proposal for pattern: 'Ambiguous Date Misinterpretation' (Significance: 6.5)\n",
      "DEBUG: Analysis cycle complete. Generated 5 proposals.\n",
      "   üéØ Generated 5 improvement proposals\n",
      "\n",
      "üìä Batch Analysis Complete\n",
      "   Conversations: 15\n",
      "   With Issues: 11\n",
      "   Total Issues: 20\n",
      "   Average Score: 0.72\n",
      "   Proposals: 5\n"
     ]
    }
   ],
   "source": [
    "\n",
    "processor = BatchPipelineProcessor()\n",
    "\n",
    "# Configure the analysis\n",
    "print(\"\\n‚öôÔ∏è Configuration:\")\n",
    "print(\"   Pattern: *.json (single agent version)\")\n",
    "print(\"   Stages: Intelligent defaults (Heuristic ‚Üí LLM ‚Üí Analysis)\")\n",
    "print(\"   Features: One-by-one progress, staged evaluation, pattern analysis\")\n",
    "\n",
    "# Run the complete batch analysis\n",
    "result_travel_v1_batch = processor.run_batch_analysis(\n",
    "    source_pattern=\"travel_v1_*.json\",\n",
    "    max_conversations=None  # Process all\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "847dee09",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'conversation_id': 'travel_v1_issue_007',\n",
       " 'evaluations': {'heuristic': EvaluatorResult(evaluator_name='heuristic', scores={'format_compliance': 1.0, 'required_fields': 1.0, 'latency_ok': 1.0}, issues=(), confidence=1.0, metadata={'total_turns': 6, 'assistant_turns': 5, 'format_issues': 0, 'field_issues': 0, 'latency_issues': 0, 'latency_ms': 0.012665987014770508}, latency_ms=0.012665987014770508),\n",
       "  'tool_call': EvaluatorResult(evaluator_name='tool_call', scores={'tool_selection': 1.0, 'param_accuracy': 1.0, 'no_hallucination': 1.0, 'execution_success': 1.0}, issues=(), confidence=0.95, metadata={'total_tool_calls': 2, 'selection_issues': 0, 'param_issues': 0, 'hallucination_issues': 0, 'execution_failures': 0, 'latency_ms': 0.01695798709988594}, latency_ms=0.01695798709988594),\n",
       "  'tool_causality': EvaluatorResult(evaluator_name='tool_causality', scores={'data_provenance': 0.5}, issues=(Issue(issue_type=<IssueType.TOOL_HALLUCINATION: 'tool_hallucination'>, severity=<IssueSeverity.HIGH: 'high'>, description=\"Tool 'hotel_search' used non-grounded parameter values: 2024-01-26. These values were never clearly mentioned in the conversation history.\", turn_id=3, details={'tool': 'hotel_search', 'hallucinated_params': ['2024-01-26'], 'context_snippet': 'Hotel search completed.'}, suggested_fix='Ensure that these specific values (2024-01-26) are either provided by the user or fetched from a previous tool before using them.'), Issue(issue_type=<IssueType.TOOL_HALLUCINATION: 'tool_hallucination'>, severity=<IssueSeverity.HIGH: 'high'>, description=\"Tool 'book_hotel' used non-grounded parameter values: 2024-01-26. These values were never clearly mentioned in the conversation history.\", turn_id=5, details={'tool': 'book_hotel', 'hallucinated_params': ['2024-01-26'], 'context_snippet': 'Booking completed.'}, suggested_fix='Ensure that these specific values (2024-01-26) are either provided by the user or fetched from a previous tool before using them.')), confidence=0.9, metadata={'total_params_checked': 4, 'hallucinations_detected': 2, 'latency_ms': 0.044208019971847534}, latency_ms=0.044208019971847534)},\n",
       " 'run_id': 'e8dc11d2-98b7-428c-b69c-e579021bac01',\n",
       " 'aggregate_score': 0.9375,\n",
       " 'issues': [Issue(issue_type=<IssueType.TOOL_HALLUCINATION: 'tool_hallucination'>, severity=<IssueSeverity.HIGH: 'high'>, description=\"Tool 'hotel_search' used non-grounded parameter values: 2024-01-26. These values were never clearly mentioned in the conversation history.\", turn_id=3, details={'tool': 'hotel_search', 'hallucinated_params': ['2024-01-26'], 'context_snippet': 'Hotel search completed.'}, suggested_fix='Ensure that these specific values (2024-01-26) are either provided by the user or fetched from a previous tool before using them.'),\n",
       "  Issue(issue_type=<IssueType.TOOL_HALLUCINATION: 'tool_hallucination'>, severity=<IssueSeverity.HIGH: 'high'>, description=\"Tool 'book_hotel' used non-grounded parameter values: 2024-01-26. These values were never clearly mentioned in the conversation history.\", turn_id=5, details={'tool': 'book_hotel', 'hallucinated_params': ['2024-01-26'], 'context_snippet': 'Booking completed.'}, suggested_fix='Ensure that these specific values (2024-01-26) are either provided by the user or fetched from a previous tool before using them.')],\n",
       " 'status': 'completed',\n",
       " 'timestamp': datetime.datetime(2025, 12, 27, 18, 13, 56, 121000)}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# result_travel_v1_batch['analysis'][1].__dict__\n",
    "result_travel_v1_batch['evaluations'][1].__dict__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b8bf3df5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "### üí° Improvement Proposals (5 generated)"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "\n",
       "#### 1. The core technical cause is the assistant's inability to handle missing or incorrectly formatted input parameters and its failure to guide users through error recovery. These interactions lack proactive clarification and fail to solicit necessary user information for successful task completion.\n",
       "\n",
       "**Evidence:** 15 conversations  \n",
       "**Type:** tool\n",
       "\n",
       "**Problem:** The updated schema introduces detailed validation rules for input parameters and provides clear descriptions and examples for users. It ensures that the assistant can better handle missing or malformed inputs by guiding the user with specific error messages and additional guidance on correcting their input.\n",
       "\n",
       "**Solution:** {\n",
       "  \"required_params\": [\n",
       "    {\n",
       "      \"name\": \"destination\",\n",
       "      \"description\": \"The airport code or city name for the flight's destination.\",\n",
       "      \"type\": \"string\",\n",
       "      \"validation\": {\n",
       "        \"n...\n",
       "---\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "\n",
       "#### 2. The AI system fails to retrieve or communicate all necessary data from its knowledge base or APIs, leading to partial responses.\n",
       "\n",
       "**Evidence:** 1 conversations  \n",
       "**Type:** prompt\n",
       "\n",
       "**Problem:** The change emphasizes the importance of gathering and verifying comprehensive information before responding. By explicitly instructing the AI to ensure completeness and verify the gathered data, we address the systemic issue of partial responses. This localized modification should prompt the AI to be more diligent in its data retrieval process.\n",
       "\n",
       "**Solution:** You are a helpful travel assistant. Ensure to gather and provide comprehensive information by retrieving all relevant data from your knowledge base or available APIs. Use tools when necessary and veri...\n",
       "---\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "\n",
       "#### 3. The core technical cause is the AI's failure to accurately interpret and prioritize user-defined budgetary and class preferences during decision-making processes.\n",
       "\n",
       "**Evidence:** 2 conversations  \n",
       "**Type:** prompt\n",
       "\n",
       "**Problem:** This modification explicitly instructs the AI to prioritize user-defined budgetary and class preferences, which addresses the core issue identified in the systemic failure pattern. By emphasizing these priorities at the beginning of the prompt, the AI is more likely to consider them throughout its decision-making process. The addition maintains existing functionality by keeping the original directive to use tools when necessary.\n",
       "\n",
       "**Solution:** You are a helpful travel assistant. Always prioritize interpreting and adhering to the user's budgetary and class preferences during decision-making. Use tools when necessary....\n",
       "---\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "\n",
       "#### 4. The AI failed to apply search filters for budget and amenities due to either a logic error in query processing or incomplete user intent extraction.\n",
       "\n",
       "**Evidence:** 1 conversations  \n",
       "**Type:** prompt\n",
       "\n",
       "**Problem:** The original prompt lacked explicit instructions for the AI to focus on extracting budget and amenities information, which are crucial for applying accurate search filters. By emphasizing the extraction of these key details, the AI is better guided to process user intents regarding budget and amenities, thereby addressing the logic error or intent extraction issue. This change remains localized and preserves the existing functionality, as it builds upon the current task without altering its core instruction.\n",
       "\n",
       "**Solution:** You are a helpful travel assistant. When understanding user requests, ensure to extract key details such as budget constraints and desired amenities. Apply appropriate search filters based on these pa...\n",
       "---\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "\n",
       "#### 5. The AI misinterprets relative date phrases like 'next week' as specific dates without verifying with the user, due to inadequate context handling.\n",
       "\n",
       "**Evidence:** 1 conversations  \n",
       "**Type:** prompt\n",
       "\n",
       "**Problem:** By explicitly instructing the AI to clarify relative date phrases with the user, we address the systemic failure of misinterpreting these phrases as specific dates. This change ensures that the AI gains the necessary context to provide accurate information, thereby fixing the issue while maintaining all existing functionalities.\n",
       "\n",
       "**Solution:** You are a helpful travel assistant. Use tools when necessary. Ensure that relative date phrases such as 'next week', 'tomorrow', etc., are clarified with the user to ensure accurate context before pro...\n",
       "---\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 4. Improvement Proposals\n",
    "from IPython.display import Markdown\n",
    "\n",
    "\n",
    "proposals = result_travel_v1_batch[\"analysis\"]\n",
    "if proposals:\n",
    "    display(Markdown(f\"### üí° Improvement Proposals ({len(proposals)} generated)\"))\n",
    "    \n",
    "    for i, proposal in enumerate(proposals, 1):\n",
    "        display(Markdown(f\"\"\"\n",
    "#### {i}. {proposal.failure_pattern}\n",
    "\n",
    "**Evidence:** {len(proposal.evidence_ids)} conversations  \n",
    "**Type:** {proposal.type.value}\n",
    "\n",
    "**Problem:** {proposal.rationale}\n",
    "\n",
    "**Solution:** {proposal.proposed_content[:200]}...\n",
    "---\n",
    "\"\"\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "12de286f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìä BATCH ANALYSIS RESULTS SUMMARY\n",
      "=============================================\n",
      "Conversations Processed: 15\n",
      "Evaluations Performed: 15\n",
      "Issues Detected: 20\n",
      "Average Score: 0.723\n",
      "Proposals Generated: 5\n",
      "\n",
      "Score Distribution:\n",
      "  Excellent (‚â•0.9): 5\n",
      "  Good (0.8-0.89): 0\n",
      "  Fair (0.7-0.79): 2\n",
      "  Poor (<0.7): 8\n",
      "\n",
      "Top 3 Proposals:\n",
      "  1. The core technical cause is the assistant's inability to handle missing or incorrectly formatted input parameters and its failure to guide users through error recovery. These interactions lack proactive clarification and fail to solicit necessary user information for successful task completion. (15 conversations)\n",
      "  2. The AI system fails to retrieve or communicate all necessary data from its knowledge base or APIs, leading to partial responses. (1 conversations)\n",
      "  3. The core technical cause is the AI's failure to accurately interpret and prioritize user-defined budgetary and class preferences during decision-making processes. (2 conversations)\n",
      "\n",
      "‚úÖ Batch analysis complete - 5 improvement proposals generated!\n",
      "üîç Now demonstrating LLM judge depth on individual conversations...\n"
     ]
    }
   ],
   "source": [
    "# Display Batch Analysis Summary\n",
    "print(\"üìä BATCH ANALYSIS RESULTS SUMMARY\")\n",
    "print(\"=\" * 45)\n",
    "\n",
    "# Extract key metrics\n",
    "summary = result_travel_v1_batch[\"summary\"]\n",
    "evaluations = result_travel_v1_batch[\"final_evaluations\"]\n",
    "proposals = result_travel_v1_batch[\"analysis\"]\n",
    "\n",
    "print(f\"Conversations Processed: {summary['total_conversations']}\")\n",
    "print(f\"Evaluations Performed: {summary['total_evaluations']}\")\n",
    "print(f\"Issues Detected: {summary['total_issues']}\")\n",
    "print(f\"Average Score: {summary['average_score']:.3f}\")\n",
    "print(f\"Proposals Generated: {summary['proposals_count']}\")\n",
    "\n",
    "# Show evaluation distribution\n",
    "scores = [e.aggregate_score for e in evaluations]\n",
    "excellent = sum(1 for s in scores if s >= 0.9)\n",
    "good = sum(1 for s in scores if 0.8 <= s < 0.9)\n",
    "fair = sum(1 for s in scores if 0.7 <= s < 0.8)\n",
    "poor = sum(1 for s in scores if s < 0.7)\n",
    "\n",
    "print(f\"\\nScore Distribution:\")\n",
    "print(f\"  Excellent (‚â•0.9): {excellent}\")\n",
    "print(f\"  Good (0.8-0.89): {good}\")\n",
    "print(f\"  Fair (0.7-0.79): {fair}\")\n",
    "print(f\"  Poor (<0.7): {poor}\")\n",
    "\n",
    "# Show top proposals\n",
    "if proposals:\n",
    "    print(f\"\\nTop {min(3, len(proposals))} Proposals:\")\n",
    "    for i, prop in enumerate(proposals[:3], 1):\n",
    "        evidence_count = len(prop.evidence_ids)\n",
    "        print(f\"  {i}. {prop.failure_pattern} ({evidence_count} conversations)\")\n",
    "\n",
    "print(f\"\\n‚úÖ Batch analysis complete - {len(proposals)} improvement proposals generated!\")\n",
    "print(\"üîç Now demonstrating LLM judge depth on individual conversations...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88262108",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "869a2c1b",
   "metadata": {},
   "source": [
    "From the batch analysis results above, we identified `travel_v1_issue_010` as a **perfect candidate** for demonstrating LLM judge capabilities:\n",
    "\n",
    "**Heuristic Evaluation Results:**\n",
    "- ‚úÖ **Score: 1.00** (technically perfect)\n",
    "- ‚úÖ **Issues: 0** (passed all checks)\n",
    "- ‚úÖ **Evaluators: 3** (only heuristic stage - no LLM needed)\n",
    "\n",
    "**What This Means:**\n",
    "- The conversation passed ALL technical validation checks\n",
    "- Tool calls were properly formatted and valid\n",
    "- No obvious format, latency, or parameter issues detected\n",
    "- **System deemed it \"good enough\" and skipped expensive LLM analysis**\n",
    "\n",
    "### What LLM Judge Should Reveal\n",
    "\n",
    "Despite passing heuristics, this conversation has **hidden semantic issues** that only deep language understanding can detect:\n",
    "\n",
    "**User Request:** \"I need flights from New York to Paris, I want business class with meal service included, departing in the morning, budget up to $1500.\"\n",
    "\n",
    "**Assistant Response:** Made valid tool call to `flight_search` with only `origin` and `destination` parameters.\n",
    "\n",
    "**Hidden Issues (Expected LLM Detection):**\n",
    "- ‚ùå **Business class preference ignored** - No `class` parameter used\n",
    "- ‚ùå **Morning departure ignored** - No time preferences considered\n",
    "- ‚ùå **Meal service ignored** - No amenity filtering\n",
    "- ‚ùå **Budget constraint ignored** - No price filtering\n",
    "- ‚ùå **Generic response** - Just \"Found 3 flights\" without details\n",
    "\n",
    "### Expected Contrast\n",
    "\n",
    "**Heuristic Evaluation:** ‚úÖ \"Technically sound API call\"\n",
    "**LLM Judge Evaluation:** ‚ùå \"Misses user intent and provides inadequate service\"\n",
    "\n",
    "This demonstrates why **staged evaluation** is crucial - technical validation alone is insufficient for quality AI agent assessment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a22c72da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LLM Score: 0.5666666666666668\n",
      "Issues Found: 2\n",
      "- low_helpfulness: The assistant did not account for the user's preferences for business class, meal service, morning departure, and budget limit.\n",
      "- low_quality: The assistant provided a generic response without addressing the specific needs of the user.\n"
     ]
    }
   ],
   "source": [
    "from src.db.repository import get_repository\n",
    "from src.evaluation.service import EvaluationService\n",
    "from src.evaluation.evaluators import get_global_registry, EvaluatorDiscovery\n",
    "\n",
    "# Initialize repository (THIS WAS MISSING!)\n",
    "repository = get_repository(data_dir=\"./data\")\n",
    "\n",
    "# Create LLM-only evaluator\n",
    "registry = get_global_registry()\n",
    "EvaluatorDiscovery.discover_and_register(registry)\n",
    "llm_service = EvaluationService(repository, registry, enabled_evaluators=[\"llm_judge\"])\n",
    "\n",
    "# Evaluate conversation that passed heuristics\n",
    "result = llm_service.evaluate(\"travel_v1_issue_010\")\n",
    "print(f\"LLM Score: {result.aggregate_score}\")\n",
    "print(f\"Issues Found: {len(result.issues)}\")\n",
    "for issue in result.issues:\n",
    "    print(f\"- {issue.issue_type.value}: {issue.description}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e029fe0",
   "metadata": {},
   "source": [
    "‚úÖ"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ai-agent-eval-E67KMgol-py3.11",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
